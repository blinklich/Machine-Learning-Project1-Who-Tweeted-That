{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/whodunnit/test_tweets_unlabeled.txt\n",
      "/kaggle/input/whodunnit/train_tweets/train_tweets.txt\n",
      "/kaggle/input/dataset1/test_tweets_unlabeled.csv\n",
      "/kaggle/input/dataset1/train_tweets.csv\n",
      "/kaggle/input/dataset/test_tweets_unlabeled.csv\n",
      "/kaggle/input/dataset/train_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Major imports\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape,BatchNormalization\n",
    "from keras.layers import Conv1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D,SeparableConv1D\n",
    "from keras.utils import to_categorical,multi_gpu_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import multiprocessing\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.merge import concatenate\n",
    "import re\n",
    "import nltk\n",
    "import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset  dataset1  whodunnit\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/\n",
    "testingData = pd.read_csv('../input/dataset1/test_tweets_unlabeled.csv')\n",
    "trainingData = pd.read_csv('../input/dataset1/train_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority of the following pre-processes text method was found online\n",
    "def pp_text(txt):\n",
    "\n",
    "    HTTP_URL_PATTERN = r'((http|ftp|https):\\/\\/)?([\\w-]+(?:(?:\\.[\\w-]{2,})+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    \n",
    "    #handles etc\n",
    "    txt = txt.strip()\n",
    "    txt = re.sub(HTTP_URL_PATTERN, ' HAVELINK ', txt)\n",
    "    txt = re.sub(r'@handle', ' ATSOMEBODY ', txt)\n",
    "    txt = re.sub(r'^RT', ' APURERETWEET ', txt)\n",
    "    txt = re.sub(r'RT(?!\\w)', ' WITHARETWEET ', txt)\n",
    "\n",
    "    #emphasise hashtag\n",
    "    current = re.search(r'#(?P<hashtag>\\w+?)\\b', txt)\n",
    "    while(current is not None):\n",
    "        hashtag = current.group('hashtag')\n",
    "        txt = re.sub(r'#'+hashtag, (' '+hashtag+' ')*1, txt)\n",
    "        current = re.search(r'#(?P<hashtag>\\w+?)\\b', txt)\n",
    "\n",
    "    #specifics\n",
    "    txt = re.sub('\\d+', ' ', txt)\n",
    "    txt = re.sub('\\s+', ' ', txt)\n",
    "    txt = re.sub('[!@#$_]', ' ', txt)\n",
    "    txt = re.sub('\\s\\W', ' ', txt)\n",
    "    txt = re.sub('\\W,\\s', ' ', txt)\n",
    "    txt = re.sub(r'\\W', ' ', txt)\n",
    "    txt = txt.lower()\n",
    "\n",
    "    #stopwords\n",
    "    pat = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    txt = pat.sub('', txt)\n",
    "\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    new_txt = \"\"\n",
    "    for word in txt.split():\n",
    "        new_txt += lemma.lemmatize(word)\n",
    "        new_txt += \" \"\n",
    "    return(new_txt.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually preprocess the data\n",
    "testingData['processed'] = testingData['tweet'].apply(pp_text)\n",
    "trainingData['processed'] = trainingData['tweet'].apply(pp_text)\n",
    "\n",
    "every_author = trainingData['author'].unique().tolist()\n",
    "i_authors = dict(enumerate(every_author))\n",
    "author_i = dict((author,i) for i, author in i_authors .items())\n",
    "trainingData['id'] = trainingData['author'].apply(lambda author: author_i[author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_count = len(i_authors )\n",
    "train_authors = to_categorical(trainingData['id'],num_classes = authors_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10000\n",
    "word_count = 30000\n",
    "limit = 250\n",
    "tokenizer = Tokenizer(num_words = word_count)\n",
    "allData = pd.concat([trainingData['processed'],testingData['processed']])\n",
    "tokenizer.fit_on_texts(allData)\n",
    "trainingSeries = tokenizer.texts_to_sequences(trainingData['processed'])\n",
    "currentTrainingData = pad_sequences(trainingSeries, maxlen = limit, padding='post')\n",
    "testingSeries = tokenizer.texts_to_sequences(testingData['processed'])\n",
    "currentTestingData = pad_sequences(testingSeries, maxlen = limit, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Begin ---\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 300)     35114100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_1 (SeparableCo (None, 250, 200)     61100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_2 (SeparableCo (None, 250, 200)     61400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_3 (SeparableCo (None, 250, 200)     61700       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 250, 200)     800         separable_conv1d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 250, 200)     800         separable_conv1d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 250, 200)     800         separable_conv1d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2, 200)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 2, 200)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 2, 200)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 600)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2, 600)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1200)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          153728      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 9297)         1199313     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,654,253\n",
      "Trainable params: 36,652,797\n",
      "Non-trainable params: 1,456\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/35\n",
      "328932/328932 [==============================] - 147s 447us/step - loss: 8.5882 - acc: 0.0064\n",
      "Epoch 2/35\n",
      "328932/328932 [==============================] - 138s 421us/step - loss: 7.8402 - acc: 0.0349\n",
      "Epoch 3/35\n",
      "328932/328932 [==============================] - 138s 419us/step - loss: 7.3090 - acc: 0.0669\n",
      "Epoch 4/35\n",
      "328932/328932 [==============================] - 138s 420us/step - loss: 6.9297 - acc: 0.0924\n",
      "Epoch 5/35\n",
      "328932/328932 [==============================] - 138s 420us/step - loss: 6.6388 - acc: 0.1112\n",
      "Epoch 6/35\n",
      "328932/328932 [==============================] - 138s 420us/step - loss: 6.4053 - acc: 0.1272\n",
      "Epoch 7/35\n",
      "328932/328932 [==============================] - 138s 420us/step - loss: 6.2095 - acc: 0.1407\n",
      "Epoch 8/35\n",
      "328932/328932 [==============================] - 138s 420us/step - loss: 6.0406 - acc: 0.1523\n",
      "Epoch 9/35\n",
      "328932/328932 [==============================] - 139s 421us/step - loss: 5.8944 - acc: 0.1629\n",
      "Epoch 10/35\n",
      " 87040/328932 [======>.......................] - ETA: 1:41 - loss: 5.6487 - acc: 0.1788"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Begin ---\")\n",
    "\n",
    "###Common CNN implementation from internet\n",
    "def cnn(words_num,embedding_dims,limit,num_class):\n",
    "    tensor_input = Input(shape=(limit,), dtype='float64')\n",
    "    embed = Embedding(words_num+1, embedding_dims)(tensor_input)\n",
    "    cnn1 = SeparableConv1D(200, 3, padding='same', strides = 1, activation='relu',kernel_regularizer = regularizers.l1(0.00001))(embed)\n",
    "    cnn1 = BatchNormalization()(cnn1)\n",
    "    cnn1 = MaxPool1D(pool_size = 100)(cnn1)\n",
    "    cnn2 = SeparableConv1D(200, 4, padding='same', strides = 1, activation='relu',kernel_regularizer = regularizers.l1(0.00001))(embed)\n",
    "    cnn2 = BatchNormalization()(cnn2)\n",
    "    cnn2 = MaxPool1D(pool_size = 100)(cnn2)\n",
    "    cnn3 = SeparableConv1D(200, 5, padding='same', strides = 1, activation='relu',kernel_regularizer = regularizers.l1(0.00001))(embed)\n",
    "    cnn3 = BatchNormalization()(cnn3)\n",
    "    cnn3 = MaxPool1D(pool_size = 100)(cnn3)\n",
    "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
    "    dropout = Dropout(0.5)(cnn)\n",
    "    flatten = Flatten()(dropout)\n",
    "    dense =  Dense(128, activation='relu')(flatten)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "    tensor_output = Dense(num_class, activation='softmax')(dropout)\n",
    "    model = Model(inputs = tensor_input, outputs = tensor_output)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_sepcnn = cnn(words_num = len(tokenizer.word_index),embedding_dims = 300,limit = limit,num_class = authors_count)\n",
    "model_sepcnn.fit(currentTrainingData,train_authors,epochs = 35, batch_size = 512)\n",
    "print('training done')\n",
    "\n",
    "pred_ = [model_sepcnn.predict(vec.reshape(1,limit)).argmax() for vec in currentTestingData]\n",
    "testingData['predicted'] = [i_authors[i] for i in pred_]\n",
    "metrics.accuracy_score(testingData['id'],testingData['predicted'])\n",
    "\n",
    "\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output succeed\n"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame({'id': range(1, testingData.shape[0] + 1),\n",
    "                       'predicted': testingData['predicted']})\n",
    "output.to_csv('./submission-Peng.csv', index = False)\n",
    "\n",
    "print(\"output succeed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
